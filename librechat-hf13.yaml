# Configuration version (required)
version: 1.0.0

# Cache settings: Set to true to enable caching
cache: true

# Definition of custom endpoints
endpoints:
  custom:
    # NagaAI
    - name: "NagaAI"
      # For `apiKey` and `baseURL`, you can use environment variables that you define.
      # recommended environment variables:
      # Known issue: you should not use `OPENROUTER_API_KEY` as it will then override the `openAI` endpoint to use OpenRouter as well.
      apiKey: "user_provided"
      baseURL: "https://api.naga.ac/v1"
      models:
        default: ["claude-2", "claude-instant", "code-llama-34b", "falcon-180b-chat", "gemini-pro", "gemini-pro-vision", "gpt-3.5-turbo", "gpt-3.5-turbo-0613", "gpt-3.5-turbo-1106", "gpt-4", "gpt-4-0613", "gpt-4-1106-preview", "gpt-4-vision-preview", "llama-2-13b-chat", "llama-2-70b-chat", "llama-2-7b-chat", "mistral-7b", "mixtral-8x7b"]
        fetch: true
      titleConvo: true
      titleModel: "gpt-3.5-turbo"
      summarize: false
      summaryModel: "gpt-3.5-turbo"
      forcePrompt: false
      modelDisplayLabel: "NagaAI"
      iconURL: "https://cdn-icons-png.flaticon.com/128/5278/5278402.png"

    # Cerebras
    - name: "Cerebras"
      apiKey: "csk-f9w9h46j83e2vfrk6kwpnc566whjy46yrfemc4erwk5k3rd6"  # ให้ผู้ใช้ป้อน API Key
      baseURL: "https://api.cerebras.ai/v1"
      models:
        default: ["llama3.1-8b", "llama3.1-70b", "llama-3.3-70b"]
        fetch: false
      titleConvo: true
      titleModel: "llama3.1-8b"
      summarize: false
      summaryModel: "llama3.1-8b"
      forcePrompt: false  # ปิดการส่ง `prompt` เพื่อใช้ `messages` แทน
      modelDisplayLabel: "Cerebras AI"
      iconURL: "https://cdn-icons-png.flaticon.com/128/8131/8131880.png"
      addParams:
        temperature: 0
        max_tokens: 100
        top_p: 1
      dropParams:
        - stop
        - temperature
        - top_p

    # DeepSeek
    - name: "DeepSeek"
      apiKey: "sk-7e65e1e268d4468a84f6d7fd4c946339"
      baseURL: "https://api.deepseek.com/v1"
      models:
        default: ["deepseek-chat"]
        fetch: false
      titleConvo: true
      titleModel: "deepseek-chat"
      summarize: false
      summaryModel: "deepseek-chat"
      forcePrompt: false
      modelDisplayLabel: "DeepSeek AI"
      iconURL: "https://cdn-icons-png.flaticon.com/128/1998/1998810.png"
      addParams:
        stream: false
      dropParams:
        - stop
        - temperature
        - top_p

    # SambaNova
    - name: "SambaNova"
      apiKey: "e2edf66e-21e2-498b-9379-1eef9378c8f5"
      baseURL: "https://api.sambanova.ai/v1"
      models:
        default:
          - "Meta-Llama-3.1-405B-Instruct"
          - "Meta-Llama-3.1-70B-Instruct"
          - "Meta-Llama-3.1-8B-Instruct"
          - "Meta-Llama-3.2-1B-Instruct"
          - "Meta-Llama-3.2-3B-Instruct"
          - "Meta-Llama-3.3-70B-Instruct"
          - "Meta-Llama-Guard-3-8B"
          - "Qwen2.5-72B-Instruct"
          - "Qwen2.5-Coder-32B-Instruct"
          - "QwQ-32B-Preview"
          - "Llama-3.2-11B-Vision-Instruct"
          - "Llama-3.2-90B-Vision-Instruct"
          - "Qwen2-Audio-7B-Instruct"
        fetch: false
      titleConvo: true
      titleModel: "Meta-Llama-3.1-8B-Instruct"
      summarize: false
      summaryModel: "Meta-Llama-3.1-8B-Instruct"
      forcePrompt: false
      modelDisplayLabel: "SambaNova AI"
      iconURL: "https://cdn-icons-png.flaticon.com/128/12122/12122348.png"
      addParams:
        stream: true
      dropParams:
        - stop
        - temperature
        - top_p

    # AIEat Siam ai
    - name: "Siamai"
      apiKey: "dummy"
      baseURL: "https://api.aieat.or.th/v1"
      models:
        default: ["."]
        fetch: false
      titleConvo: true
      titleModel: "."
      summarize: false
      summaryModel: "."
      forcePrompt: true
      modelDisplayLabel: "AIEat"
      iconURL: "https://cdn-icons-png.flaticon.com/128/10598/10598728.png"
      addParams:
        max_tokens: 512
        temperature: 0.7
        top_p: 0.8
        top_k: 40
        stop:
          - "<|im_end|>"
          - "||"
          - "Assistant:"
          - "Human:"
      dropParams:
        - stop
        - temperature
        - top_p

    # OpenTyphoon.ai API
    - name: "OpenTyphoon"
      apiKey: "sk-BET4yZq8hG3uoUwo2ilOyMYEhoXk1Uel3Gz9v5VpGMk6Hio7"  # Replace with your actual API key
      baseURL: "https://api.opentyphoon.ai/v1/chat/completions"
      models:
        default:
          - "typhoon-instruct"
          - "typhoon-v1.5-instruct"
          - "typhoon-v1.5x-70b-instruct"
          - "typhoon-v2-8b-instruct"
          - "typhoon-v2-70b-instruct"
        fetch: false
      addParams:
        max_tokens: 512
        temperature: 0.6
        top_p: 0.95
        repetition_penalty: 1.05
        stream: false
      dropParams:
        - stop
        - metadata
        - store
      titleConvo: true
      modelDisplayLabel: "OpenTyphoon AI"
      iconURL: "https://cdn-icons-png.flaticon.com/128/8484/8484072.png"

    # Groq AI
    - name: "Groq"
      apiKey: "GROQ_API_KEY"  # Replace with your actual API key
      baseURL: "https://api.groq.com/openai/v1"
      models:
        default:
          - "llama-3.3-70b-versatile"
          - "llama-3.1-8b-instant"
          - "llama-guard-3-8b"
          - "llama3-70b-8192"
          - "llama3-8b-8192"
          - "mixtral-8x7b-32768"
        fetch: false
      titleConvo: true
      titleModel: "llama-3.3-70b-versatile"
      summarize: false
      summaryModel: "llama-3.3-70b-versatile"
      forcePrompt: false
      modelDisplayLabel: "Groq AI"
      iconURL: "https://cdn-icons-png.flaticon.com/128/888/888843.png"
      addParams:
        temperature: 0.7
        max_tokens: 512
        top_p: 0.9
      dropParams:
        - stop
        - temperature
        - top_p
        
    # Hugging Face Microsoft Phi-4 API
    - name: "Hugging Face"
      apiKey: "user_provided"
      baseURL: "https://api-inference.huggingface.co/v1"
      models:
        default:
          - "PowerInfer/SmallThinker-3B-Preview"
          - "impira/layoutlm-document-qa"
          - "mistralai/Mistral-Nemo-Instruct-2407"
          - "mistralai/Mixtral-8x7B-Instruct-v0.1"
          - "NousResearch/Hermes-3-Llama-3.1-8B"
          - "CodeLlama/CodeLlama-34B-Instruct-hf"
          - "meta-llama/Llama-3.1-8B-Instruct"
          - "meta-llama/Llama-3.2-1B-Instruct"
          - "meta-llama/Llama-3.2-3B-Instruct"
          - "meta-llama/Llama-3.3-70B-Instruct"
          - "tiiuae/falcon-7b-instruct"
          - "Qwen/Qwen2.5-Coder-32B-Instruct"
          - "Qwen/Qwen2.5-72B-Instruct"
          - "Qwen/QwQ-32B-Preview"
          - "microsoft/Phi-3.5-mini-instruct"
          - "01-ai/Yi-1.5-34B-Chat"
          - "HuggingFaceH4/starchat2-15b-v0.1"
        fetch: false
      titleConvo: true
      titleModel: "NousResearch/Hermes-3-Llama-3.1-8B"
      summarize: false
      summaryModel: "NousResearch/Hermes-3-Llama-3.1-8B"
      forcePrompt: false
      modelDisplayLabel: "Hugging Face "
      iconURL: "https://huggingface.co/favicon.ico"
      addParams:
        max_tokens: 500
        stream: true
      dropParams:
        - stop
        - temperature
        - top_p

# See the Custom Configuration Guide for more information:
# https://docs.librechat.ai/install/configuration/custom_config.html
